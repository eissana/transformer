{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eissana/transformer/blob/master/shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Init"
      ],
      "metadata": {
        "id": "LOrRSB_MWjfc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dds5EpUecQ3",
        "outputId": "6c1c973a-73b0-48e4-81a9-65e1743fb218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.request import urlopen\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"running on: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Data"
      ],
      "metadata": {
        "id": "L4JW3Z4JWqM3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytabp3MRejPz"
      },
      "outputs": [],
      "source": [
        "with urlopen('https://raw.githubusercontent.com/eissana/transformer/master/data/shakespeare.txt') as f:\n",
        "    words = f.read().decode('utf-8')\n",
        "print(f'First few lines:\\n {words[:200]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23cmXq3ofCPa"
      },
      "outputs": [],
      "source": [
        "print(f'Number of characters: {len(words)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2V_C8RpfXIY"
      },
      "outputs": [],
      "source": [
        "chars = sorted(set(words))\n",
        "nchars = len(chars)\n",
        "# includes \\n (new line), space, numbers and letters.\n",
        "print(f'{nchars = }')\n",
        "','.join(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYh4ZcQSf1jy"
      },
      "outputs": [],
      "source": [
        "atoi = {c: i for i, c in enumerate(chars)}\n",
        "itoa = {i: c for i, c in enumerate(chars)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2N6VyPpf4g3"
      },
      "outputs": [],
      "source": [
        "def encode(str):\n",
        "    return [atoi[c] for c in str]\n",
        "\n",
        "def decode(arr):\n",
        "    return ''.join([itoa[i] for i in arr])\n",
        "\n",
        "str = words[:104]\n",
        "enc = encode(str)\n",
        "print(enc)\n",
        "print(decode(enc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dLiYuZRgKHz"
      },
      "outputs": [],
      "source": [
        "encdata = torch.tensor(encode(words), dtype=torch.long)\n",
        "print(encdata[:104])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-R6BdPggTps"
      },
      "outputs": [],
      "source": [
        "nencdata = len(encdata)\n",
        "ntrain, nvalid = int(0.8*nencdata), int(0.1*nencdata)\n",
        "\n",
        "data = {'train': encdata[:ntrain],\n",
        "        'valid': encdata[ntrain:ntrain+nvalid],\n",
        "        'test': encdata[ntrain+nvalid:]}\n",
        "\n",
        "print('Data split: ', end='')\n",
        "print(f\"{nencdata} = {len(data['train'])} + {len(data['valid'])} + {len(data['test'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Parameters"
      ],
      "metadata": {
        "id": "Ey0LHGP8Wx9p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFAJZM81gZEK"
      },
      "outputs": [],
      "source": [
        "nembd = 384\n",
        "nhead = 6\n",
        "nblock = 6\n",
        "dropout = 0.3\n",
        "\n",
        "nbatch = 64\n",
        "block_size = 256\n",
        "\n",
        "nepoch = 5000\n",
        "eval_size = 500\n",
        "lr = 3e-4\n",
        "\n",
        "model_filename = 'multihead_transformer.pt'\n",
        "final_losses = 'final_losses.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Functions"
      ],
      "metadata": {
        "id": "_bxhKc14W2az"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_v35XLXQgnfc"
      },
      "outputs": [],
      "source": [
        "def get_batch(data, nbatch, block_size):\n",
        "    '''\n",
        "    Generates a batch of examples.\n",
        "    (x[i], y[i]) is a pair of consecutive characters in the text.\n",
        "    '''\n",
        "    indices = torch.randint(len(data)-block_size, (nbatch,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in indices]).to(device)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in indices]).to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFNAPLHxgzND"
      },
      "outputs": [],
      "source": [
        "x, y = get_batch(data['train'], nbatch=2, block_size=8)\n",
        "x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xwn0d26ag16A"
      },
      "outputs": [],
      "source": [
        "def get_loss(logits, y):\n",
        "    '''\n",
        "    Computes cross-entropy loss, given logits and labels.\n",
        "    '''\n",
        "    B, T, C = logits.shape\n",
        "    # F.cross_entropy expects size C, (B, C), or (B, C, ...)\n",
        "    # logits shape is (B, T, C), so we flatten the first two dimensions.\n",
        "    return F.cross_entropy(logits.view(B*T, C), y.view(B*T))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBNskdNsg4sJ"
      },
      "outputs": [],
      "source": [
        "def train(model, data, nepoch=100, nbatch=20, block_size=8, lr=1e-3, losses=None):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    if losses is None:\n",
        "        losses = {\n",
        "            'train': [],\n",
        "            'valid': [],\n",
        "        }\n",
        "    for _ in range(nepoch):\n",
        "        x, y = get_batch(data['train'], nbatch=nbatch, block_size=block_size)\n",
        "\n",
        "        logits = model(x)\n",
        "        loss = get_loss(logits, y)\n",
        "        losses['train'].append(loss.item())\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x, y = get_batch(data['valid'], nbatch=nbatch, block_size=block_size)\n",
        "            logits = model(x)\n",
        "            vloss = get_loss(logits, y)\n",
        "            losses['valid'].append(vloss.item())\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3uJ5r5lg8Bd"
      },
      "outputs": [],
      "source": [
        "def gen_text(model, block_size, max_size=100):\n",
        "    '''\n",
        "    Generates text using the model starting from nothing.\n",
        "    '''\n",
        "    # starting from '\\n' char we generate text.\n",
        "    x = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "    for _ in range(max_size):\n",
        "        logits = model(x[:, -block_size:])\n",
        "        # only consider the last logit\n",
        "        logits = logits[:, -1, :]\n",
        "        score = F.softmax(logits, dim=1)\n",
        "        next_token = score.multinomial(1)\n",
        "        x = torch.cat((x, next_token), dim=1)\n",
        "    return x[0].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOwOwp4nhAi5"
      },
      "outputs": [],
      "source": [
        "def nparameters(model):\n",
        "    '''\n",
        "    Returns the total number of model parameters.\n",
        "    '''\n",
        "    return sum([p.nelement() for p in model.parameters()])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Transformer Model"
      ],
      "metadata": {
        "id": "zlsvU-HbW7TE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlbJwTFShIzG"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    '''\n",
        "    Self-attention head layer.\n",
        "    '''\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(nembd, head_size, bias=False)\n",
        "        self.key = nn.Linear(nembd, head_size, bias=False)\n",
        "        self.value = nn.Linear(nembd, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # tril is not a model parameter so we register it as a buffer.\n",
        "        # block_size is the maximum size. The actual size can be smaller.\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, T, C = x.shape\n",
        "        query = self.query(x)\n",
        "        key = self.key(x)\n",
        "        weights = query @ key.transpose(-2, -1) * C**-0.5\n",
        "\n",
        "        # The time dimension can be smaller than the block-size.\n",
        "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        weights = F.softmax(weights, dim=-1)\n",
        "        weights = self.dropout(weights)\n",
        "\n",
        "        value = self.value(x)\n",
        "        out = weights @ value\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cM9yxFOqhd2k"
      },
      "outputs": [],
      "source": [
        "class MultiHead(nn.Module):\n",
        "    def __init__(self, nhead, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(nhead)])\n",
        "        self.proj = nn.Linear(nembd, nembd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhyqwLZuhgpf"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, fain_in, fan_out):\n",
        "        super().__init__()\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(fain_in, 4 * fan_out),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * fan_out, fan_out),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.ff(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBsNdyC7hh5t"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, nembd, nhead):\n",
        "        super().__init__()\n",
        "        # self-attention heads\n",
        "        self.sa = MultiHead(nhead, nembd//nhead)\n",
        "        self.sa_layer_norm = nn.LayerNorm(nembd)\n",
        "        # feed-forward network\n",
        "        self.ff = FeedForward(nembd, nembd)\n",
        "        self.ff_layer_norm = nn.LayerNorm(nembd)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        # deep residual network\n",
        "        out = out + self.sa(self.sa_layer_norm(out))\n",
        "        out = out + self.ff(self.ff_layer_norm(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8l12t-rkhvDR"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    '''\n",
        "    A simple transformer model for building a character-level\n",
        "    language model using multiple self-attention heads.\n",
        "    '''\n",
        "    def __init__(self, nhead, nblock):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(nchars, nembd)\n",
        "        self.position_emb = nn.Embedding(block_size, nembd)\n",
        "        self.blocks = nn.Sequential(*[Block(nembd, nhead) for _ in range(nblock)])\n",
        "        self.layer_norm = nn.LayerNorm(nembd)\n",
        "        self.linear = nn.Linear(nembd, nchars)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, T = x.shape\n",
        "        # x.shape == (nbatch, block_size, nchars)\n",
        "        # token_emb.shape == (nbatch, block_size, nembd)\n",
        "        token_emb = self.token_emb(x)\n",
        "        # position_emb.shape == (nbatch, block_size, nembd)\n",
        "        position_emb = self.position_emb(torch.arange(T, device=device))\n",
        "        # out.shape == (nbatch, block_size, nembd)\n",
        "        out = token_emb + position_emb\n",
        "        # out.shape == (nbatch, block_size, nembd)\n",
        "        out = self.blocks(out)\n",
        "        out = self.layer_norm(out)\n",
        "        # out.shape == (nbatch, block_size, nchars)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "multihead_transformer = Transformer(nhead, nblock).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTxKjbkghx77"
      },
      "outputs": [],
      "source": [
        "nparameters(multihead_transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gbRUI8hh1uJ"
      },
      "outputs": [],
      "source": [
        "multihead_losses = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYqwpgc8h5x2"
      },
      "outputs": [],
      "source": [
        "multihead_losses = train(multihead_transformer, data, nepoch, nbatch, block_size, lr=lr, losses=multihead_losses)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"final training loss: {np.mean(multihead_losses['train'][-eval_size:])}\")\n",
        "print(f\"final validation loss: {np.mean(multihead_losses['valid'][-eval_size:])}\")"
      ],
      "metadata": {
        "id": "tn63To28bPgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(multihead_transformer, model_filename)\n",
        "\n",
        "with open(final_losses, 'w') as f:\n",
        "  f.write(f\"final training loss: {np.mean(multihead_losses['train'][-eval_size:])}\\n\")\n",
        "  f.write(f\"final validation loss: {np.mean(multihead_losses['valid'][-eval_size:])}\\n\")"
      ],
      "metadata": {
        "id": "C0vUFUwnL__o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoGps7nqh9vk"
      },
      "outputs": [],
      "source": [
        "!cat $final_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NtIAXyTiHE0"
      },
      "outputs": [],
      "source": [
        "plt.plot(torch.tensor(multihead_losses['train']).view(-1, eval_size).mean(axis=1));\n",
        "plt.plot(torch.tensor(multihead_losses['valid']).view(-1, eval_size).mean(axis=1));\n",
        "plt.legend(['training loss', 'validation loss']);"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multihead_transformer = torch.load(model_filename)"
      ],
      "metadata": {
        "id": "uVuIz9hOMZy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TC_4T1f8iq_a"
      },
      "outputs": [],
      "source": [
        "print(decode(gen_text(multihead_transformer, block_size, max_size=1000)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPsmzvi+Y7PNvMm2T1gen6d",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}